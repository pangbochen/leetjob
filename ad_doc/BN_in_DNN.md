
https://zhuanlan.zhihu.com/p/43200897

BN在深度学习中的大量应用

# 一.从Mini-Batch SGD说起

一般提到的SGD是指的Mini-batch SGD

# 二．Normalization到底是在做什么
对于规范化的相关理解

所谓规范化，是希望转换后的数值 \bar{x} 满足一定的特性，至于对数值具体如何变换，跟规范化目标有关，也就是说f()函数的具体形式，不同的规范化目标导致具体方法中函数所采用的形式不同。


深度学习中的Normalization
- 因为神经网络里主要有两类实体：神经元或者连接神经元的边，所以按照规范化操作涉及对象的不同可以分为两大类，
- 一类是对第L层每个神经元的激活值或者说对于第L+1层网络神经元的输入值进行Normalization操作
  - BatchNorm/LayerNorm/InstanceNorm/GroupNorm
- 一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，
  - Weight Norm
  - 对参数的的L1/L2等正则项

L1与L2
- L1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果
- L2正则的规范化目标是有效减小原始参数值的大小


对神经元的规范化操作方法

    目前神经网络中常见的第一类Normalization方法包括Batch Normalization/Layer Normalization/Instance Normalization和Group Normalization，BN最早由Google研究人员于2015年提出，后面几个算法算是BN的改进版本。不论是哪个方法，其基本计算步骤都如上所述，大同小异，最主要的区别在于神经元集合S的范围怎么定，不同的方法采用了不同的神经元集合定义方法。

不同的Norm阿里咋提on差别在哪里
- 这类深度学习的规范化目标是将神经元的激活值 a_{i} 限定在均值为0方差为1的正态分布中。
- 必须有一定的手段求出均值和方差
  - 需要定义一个集合范围才能进行计算

# 三.Batch Normalization如何做

将常见的结构分为 MLP， CNN和RNN

1，MLP中的BN操作

  n个同一个神经元被Batch不同训练实例激发的激活值。划定集合S的范围后，Normalization的具体计算过程与前文所述计算过程一样，采用公式3即可完成规范化操作。

2.CNN中的BN


### BN存在的问题：
1.如果Batch Size太小，则BN效果明显下降。
- 小batch数据样本过少，得不到有效统计量，噪声过大
- 比如BN无法应用在Online Learning中，因为在线模型是单实例更新模型参数的，难以组织起Mini-Batch结构。

2.对于有些像素级图片生成任务来说，BN效果不佳
- 图片风格转换等应用场景
- Mini-Batch内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。

3.RNN等动态网络使用BN效果不佳且使用起来不方便

4.局限4：训练时和推理时统计量不一致


上述的问题主要在于
- BN要求计算统计量的时候必须在同一个Mini-Batch内的实例之间进行统计
  - Batch内实例之间的相互依赖和影响的关系

自然而然的解决方法

    把对Batch的依赖去掉，转换统计集合范围。在统计均值方差的时候，不依赖Batch内数据，只用当前处理的单个训练数据来获得均值方差的统计量，这样因为不再依赖Batch内其它训练数据，那么就不存在因为Batch约束导致的问题。


# 四.Layer Normalization、Instance Normalization及Group Normalization

## layer normalization
背景：只有当前一个训练实例的情形下，也能够找到一个合理的统计范围

直接用同层隐层神经元的响应值作为集合S的范围来求均值和方差

- MLP的同一隐层自己包含了若干神经元；
- CNN中同一个卷积层包含k个输出通道，每个通道包含m*n个神经元，整个通道包含了k*m*n个神经元；
- RNN的每个时间步的隐层也包含了若干神经元。

神经元的响应值作为layernorm

## Instance Normalization

和layer normalization相似的思想，但是进行范围缩小

对于CNN，缩小到单个卷积核的内部

    对于CNN明显是可以的，因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到单个卷积核对应的输出通道内部。

RNN和MLP是无法进行Instance Normalization操作

## group normalization
- Layer Normalization是将同层所有神经元作为统计范围，
- Instance Normalization则是CNN中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围。

介于中间的分组思想
- 通道分组是CNN常用的模型优化技巧
- 

Group Normalization在要求Batch Size比较小的场景下或者物体检测／视频分类等应用场景下效果是优于BN的。


## 小故事来总结

理发师与理发馆的故事

    很久很久以前，在遥远的L国内有一个神奇的理发馆，理发馆里面有很多勤劳的理发师，来这里理发的顾客也很奇特，他们所有人都会要求理发师（神经元）理出和其他人差不多长的头发（求均值）。那么和其他人差不多长究竟是多长呢？这可难不倒我们可爱又聪明的理发师，于是理发师把自己最近24个小时服务过的顾客（Mini-Batch）进入理发店时的头发长度求个平均值，这个均值就是“和其他人差不多长”的长度。来这里的每个顾客都很满意，时间久了，人们尊称这些理发师为：BatchNorm理发师。

    不幸总是突然的，有一天，理发馆里发生了一件怪事，所有理发师的记忆只能维持1分钟，他们再也记不住过去24小时中发生的事情了，自然也记不住过去服务客人的头发长度。但是每个顾客仍然要求剪出和其他人差不多长的头发长度，这可怎么办？聪明的理发师们又想出了一个办法：他们相互大声报出同一时刻在理发馆里自己手上客人的头发长度，每个理发师就可以用这些人的头发长度均值作为满足自己手上客人条件的长度。尽管这是一群得了失忆综合证的理发师，但是顾客对他们的服务仍然非常满意，于是人们改称他们为：LayerNorm理发师。

    不幸总是突然的，有一天，理发馆里又发生了一件怪事，理发师们不仅得了失忆症，这次都突然失聪，再也听不到其它理发师的口头通知，而固执的客人仍然坚持要理出“和其他人差不多长”的头发。对了，忘了介绍了，理发馆是有隔间的，每个隔间有K个理发师同时给顾客理发，虽然我们可爱又聪明的理发师现在失忆又失聪，但是再大的困难也难不倒也叫不醒这群装睡的人，他们醒来后群策群力又发明了一个新方法：同一个隔间的理发师通过相互打手势来通知其它理发师自己手上顾客的头发长度。于是同一个隔间的理发师又可以剪出顾客满意的头发了。人们称这些身残志坚的理发师为：GroupNorm理发师。

    不幸总是突然的，有一天，理发馆里又发生了一件怪事，不过这次不是天灾是人祸，理发馆老板出于好心，给每位理发师单独开个办公室给顾客理发，但是好心办了坏事，这样一来，失忆失聪又无法相互打手势的理发师们怎么应对顽固的顾客呢？怎样才能继续理出“和其他人差不多长”的头发呢？想必一般人这个时候基本无路可走了，但是我们可爱又聪明，同时失聪又失忆的理发师仍然想出了解决办法：他们看了看客人头上的头发，发现不同地方有长有短，于是就把同一个客人所有头发的平均长度作为难题的答案（CNN的InstanceNorm）。听起来这个解决方案匪夷所思，但是出人意料的是，有些客人居然仍然非常满意。人们管这些传说中的神奇理发师为：InstanceNorm理发师。

# 五.Normalization操作的Re-Scaling不变性
BN也具备权重矩阵Re-Scaling不变性


# 六.Batch Normalization为何有效

BN在提出后获得了巨大的成功，目前在各种深度学习场景下广泛应用，因为它能加快神经网络收敛速度，不再依赖精细的参数初始化过程，可以使用较大的学习率等很多好处

原始论文的解释为：BN可以解决神经网络训练过程中的ICS（Internal Covariate Shift）

所谓ICS问题，指的是由于深度网络由很多隐层构成，在训练过程中由于底层网络参数不断变化，导致上层隐层神经元激活值的分布逐渐发生很大的变化和偏移，而这非常不利于有效稳定地训练神经网络

但其实BN和ICS没有什么关系


BN通过参数重整确实起到了平滑损失曲面及梯度的作用。

Re-Scaling特性和Loss曲面平滑作用是Normalization的一体两面

# 总结

本文归纳了目前深度学习技术中针对神经元进行Normalization操作的若干种模型，可以看出，所有模型都采取了类似的步骤和过程，将神经元的激活值重整为均值为0方差为1的新数值，最大的不同在于计算统计量的神经元集合S的划分方法上。BN采用了同一个神经元，但是来自于Mini-Batch中不同训练实例导致的不同激活作为统计范围。而为了克服Mini-Batch带来的弊端，后续改进方法抛弃了Mini-Batch的思路，只用当前训练实例引发的激活来划分集合S的统计范围，概括而言，LayerNorm采用同隐层的所有神经元；InstanceNorm采用CNN中卷积层的单个通道作为统计范围，而GroupNorm则折衷两者，采用卷积层的通道分组，在划分为同一个分组的通道内来作为通道范围。


至于各种Normalization的适用场景，可以简洁归纳如下：对于RNN的神经网络结构来说，目前只有LayerNorm是相对有效的；如果是GAN等图片生成或图片内容改写类型的任务，可以优先尝试InstanceNorm；如果使用场景约束BatchSize必须设置很小，无疑此时考虑使用GroupNorm；而其它任务情形应该优先考虑使用BatchNorm。



