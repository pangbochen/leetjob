来自于Batch Normalization的导读


Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift。

https://zhuanlan.zhihu.com/p/38176412


BN本身来解决“Internal Covariate Shift”问题

# “Internal Covariate Shift”
论文首先说明Mini-Batch SGD相对于One Example SGD的两个优势：梯度更新方向更准确；并行计算速度快；（本文作者：为啥要说这些？因为BatchNorm是基于Mini-Batch SGD的，所以先夸下Mini-Batch SGD，当然也是大实话）；

covariate shift的概念：如果ML系统实例集合<X,Y>中的输入值X的分布老是变，这不符合IID假设啊，那您怎么让我稳定的学规律啊，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。

对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数老在变，所以每个隐层都会面临covariate shift的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。

BN的思想本身来自于图像中的白化处理


## 来自于本质的思想

训练深层神经网络收敛越来越慢的本质原因

    因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致后向传播时低层神经网络的梯度消失，

BN通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正太分布
- 这样使得激活输入值落在非线性函数对输入比较敏感的区域，
- 这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。

## 针对mini-batch SGD的BN操作

某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。


# BatchNorm的好处

提升了训练速度，收敛过程大大加快，还能增加分类效果，

对效果的解释
- 类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果
- 调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等



# 同样的来自于评论区的讨论

1.等价的，相当于引入了adaptive weak nonlinear function， 就是一方面让非线性部分具备一定的对数据的自适应性，另一方面又保证了弱的非线性。这两条当然可以提高系统的稳定性和效率，这保证了在不同维度上信息流的一致性以及整体结构上的平滑性。

2.实际上是在保证系统中每个局部操作的一致性，使得在每个局部上大致以相同的速率丢弃部分信息。这个和量子多体系统中entanglement renormalization相似，不过后者里面由于是对量子态操作，而量子态自身就是归一化的，所以每次操作都能大致均匀的抛弃部分局部信息，这里也需要完成一个归一化。

