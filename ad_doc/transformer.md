https://zhuanlan.zhihu.com/p/54743941

放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较

继续的文章探索

作者的两个巨大的判断
- Bert这种两阶段的模式（预训练+Finetuning）必将成为NLP领域研究和工业应用的流行方法
- 从NLP领域的特征抽取器角度来说，Transformer会逐步取代RNN成为最主流的的特征抽取器

对NLP领域的三大特征抽取器的命运判断
- RNN人老珠黄，已经基本完成它的历史使命，将来会逐步退出历史舞台；
- CNN如果改造得当，将来还是有希望有自己在NLP领域的一席之地，如果改造成功程度超出期望，那么还有一丝可能作为割据一方的军阀，继续生存壮大，当然我认为这个希望不大，可能跟宋小宝打篮球把姚明打哭的概率相当；
- 而新欢Transformer明显会很快成为NLP里担当大任的最主流的特征抽取器。


# 战场侦查：NLP任务的特点及任务类型
NLP任务的特点：从特征抽取器到具体的预测阶段
- NLP的输入往往是一句话或者一篇文章
  - 一维线性序列
  - 输入不定长
  - 单词或者子句的相对位置关系很重要
  - 句子中的长距离特征对于理解语义也非常关键
上述的特点对NLP特征抽取器提出了巨大的挑战

一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性。


对于NLP任务进行定义
- 序列标注：分词、POS Tag、NER、语义标注
  - 句子中每个单词要求模型根据上下文都要给出一个分类类别
- 文本分类：文本分类、情感计算
  - 不管文章有多长，总体给出一个分类类别即可
- 句子关系判断：entailment，QA，自然语言推理，语义改写
  - 给定两个句子，模型判断出两个句子是否具备某种语义关系
- 生成式任务：机器翻译、文本摘要
  - 输入文本内容后，需要自主生成另外一段文字。

模型层面上什么重要呢？
- 特征抽取器的能力
- 那么怎么才能成为算法高手？你去设计一个更强大的特征抽取器呀。

# 沙场老将RNN：廉颇老矣，尚能饭否

隐层节点之间形成了线性序列，采取线性序列结构不断从前往后收集输入信息

## 核心问题，为什么RNN成为了NLP问题的主流特征抽取器？
- 反向传播的时候存在优化困难的问题
  - 反向传播路径太长，容易导致严重的梯度消失或梯度爆炸问题。
  - 引入LSTM和GRU模型
    - 增加中间状态信息直接向后传播，以此缓解梯度消失问题
- 借鉴了attention机制（从CV领域）
- 叠加变成深层网络
  - 多层RNN叠加
  - 双向RNN
- Seq2Seq框架

核心的原因是**RNN的结构天然适配解决NLP的问题，NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。所以RNN特别适合NLP这种线形序列应用场景，这是RNN为何在NLP界如此流行的根本原因。**

## 两个严重的问题
2014-2018在NLP界的当红

第一个原因在于一些后起之秀新模型的崛起
- 经过特殊改造的CNN模型，以及最近特别流行的Transformer，



另外一个严重阻碍RNN将来继续走红的问题是：RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。
- 不能高效并行

- T时刻隐层状态的计算，依赖两个输入，一个是T时刻的句子输入单词Xt
- T时刻的隐层状态St还依赖T-1时刻的隐层状态S(t-1)的输出，这是最能体现RNN本质特征的一点
- 老老实实地按着时间步一个单词一个单词往后走。
- 两种改进（悲观的改进）
  - CNN模型化，Sliced RNN
  - SRU


# 偏师之将CNN：刺激战场绝地求生

每个字符的Word Embedding的长度为d，那么输入就是d*n的二维向量。

通过embedding表征，一维变成二维状态

原始版本的问题
- CNN有window的设置（滑动窗口）
- 单卷积层无法捕捉远距离特征
  - 如果滑动窗口k最大为2，而如果有个远距离特征距离是5，那么无论上多少个卷积核，都无法覆盖到长度为5的距离的输入，所以它是无法捕获长距离特征的。

Dilated CNN远程捕捉特征
- 一种是假设我们仍然用单个卷积层，滑动窗口大小k假设为3，就是只接收三个输入单词，但是我们想捕获距离为5的特征，怎么做才行？
  - 跳着覆盖
  - Dilated 卷积的基本思想
- 把深度做起来。第一层卷积层，假设滑动窗口大小k是3，如果再往上叠一层卷积层，假设滑动窗口大小也是3，但是第二层窗口覆盖的是第一层窗口的输出特征，所以它其实能覆盖输入的距离达到了5。如果继续往上叠加卷积层，可以继续增大卷积核覆盖输入的长度

目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度，这背后是有原因的（当然图像领域也是这个趋势）。
- 但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。

想方设法把CNN的深度做起来，随着深度的增加，很多看似无关的问题就随之解决了。就跟我们国家最近40年的主旋律是发展经济一样，经济发展好了，很多问题就不是问题了。最

# 白衣骑士Transformer：盖世英雄站上舞台

最早在 Attention is all you need的论文中提出来

论文中说的的Transformer指的是完整的Encoder-Decoder框架，而我这里是从特征提取器角度来说的，你可以简单理解为论文中的Encoder部分。

为什么？
- encoder部分目的比较单纯
- decoder的部分功能很多：特征提取+语言模型

transformer block
- self-attention
  - multi-head attention 
  - skip connection
- add and normalization
- feed forward
- add and normalization

首先，自然语言一般是个不定长的句子，那么这个不定长问题怎么解决呢？
- Transformer做法跟CNN是类似的，一般设定输入的最大长度，如果句子没那么长，则用Padding填充，这样整个模型输入起码看起来是定长的了。


另外，NLP句子中单词之间的相对位置是包含很多信息的，
- RNN因为结构就是线性序列的，
- CNN的卷积层其实也是保留了位置相对信息的
- Transformer不像RNN或CNN，必须明确的在输入端将Positon信息编码，Transformer是用位置函数来进行位置编码的，   
  - 将单词embedding和单词对应的position embedding加起来形成单词的输入embedding，

NLP句子中长距离依赖特征的问题
- Self attention天然就能解决这个问题，因为在集成信息的时候，当前单词和句子中任意单词都发生了联系，所以一步到位就把这个事情做掉了。

deploy的两种版本
- transformer base 和 transformer big
  - 区别在于包含的transformer的block的数量不同

## 进行三者的对比

语义特征提取能力
- Transformer在这方面的能力非常显著地超过RNN和CNN（在考察语义类能力的任务WSD中，Transformer超过RNN和CNN大约4-8个绝对百分点），RNN和CNN两者能力差不太多。


长距离特征捕获能力
- 原生CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，能力由强到弱排序为Transformer>RNN>>CNN; 
- 但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者。



Multi-head attention的head数量严重影响NLP任务中Long-range特征捕获能力：结论是head越多越有利于捕获long-range特征。

个差异是由于两个论文中的实验中Transformer的超参设置不同导致的，其中尤其是multi-head的数量，对结果影响严重，而如果正确设置一些超参，那么之前Trans的论文结论是不成立的。也就是说，我们目前仍然可以维持下面结论：在远距离特征捕获能力方面，Transformer和RNN能力相近，而CNN在这方面则显著弱于前两者。

任务综合特征抽取能力




# 三者的合流: 向Transformer靠拢

比如讲 self-attention模块用双向RNN或者CNN替换掉

这说明Transformer之所以能够效果这么好，不仅仅multi-head attention在发生作用，而是几乎所有构件都在共同发挥作用，是一个小小的系统工程

RNN和CNN的大的出路在于寄生到Transformer Block里，这个原则没问题，看起来也是他俩的唯一出路。但是，要想效果足够好，在塞进去的RNN和CNN上值得花些功夫，需要一些新型的RNN和CNN模型，以此来配合Transformer的其它构件，共同发挥作用。如果走这条路，那么RNN和CNN翻身的一天也许还会到来。

# 2019来自未来的消息：总结

进退维谷的RNN
- 原生的RNN还有一个致命的问题：并行计算能力受限制太严重。
- RNN福音派的错觉
  - 我看到网上很多人还在推RNN说：其实还是RNN好用。我觉得这其实是一种错觉。之所以会产生这个错觉，原因来自两个方面：一方面是因为RNN发展历史长，所以有大量经过优化的RNN框架可用，这对技术选型选择困难症患者来说是个福音，因为你随手选一个知名度还可以的估计效果就不错，包括对一些数据集的前人摸索出的超参数或者调参经验
- Bert两阶段训练模型，那么对于小数据集合来说会极大缓解效果问题


一希尚存的CN
- 起初落后，未来比RNN好
  - 本来CNN做NLP天然的一个缺陷：无法有效捕获长距离特征的问题，就得到了极大缓解
  - 早期的CNN做不好NLP的一个很大原因是网络深度做不起来，

那我们把CNN引到Transformer结构里，比如代替掉Self attention，这样和Transformer还有一战吧？嗯，是的，目前看貌似只有这条路是能走的通的，引入depth separate CNN可以达到和Transformer接近的效果。但是，我想问的是：你确认长成这样的CNN，就是把CNN塞到Transformer Block的肚子里，你确认它的亲朋好友还能认出它吗？

CNN塞到Transformer肚子里这种方案，对于篇章级别的NLP任务来说，跟采取self attention作为发动机的Transformer方案对比起来，是具有极大优势的领域，



# 惯例的评论区时间

1.CNN感觉没什么无法克服的困难，追上transformer的可能性是有的
2.transformer如果把position encoding 去掉的话会有什么问题
- 这个我在翻译试过。encoder的影响会非常剧烈（BLEU掉10点），但是decoder的影响不太严重（1，2点，因为decoder的生成autoregressive已经有类似RNN的属性了）。

3.问大佬一个问题：多层全链接网络比transformer差的地方在于什么呢？是搜索空间太大所以收敛更慢么？但是transformer的参数量比一个小型的多层全链接多很多吧，为什么收敛会更快呢？
- 参数量太大，要比较相同层深的参数量，不能拿深层和浅层的比

4.大佬我有一个疑问，看<attention is all you need>文章中，position encoding直接和embedding层相加，但是position的值是（-1，1），而embedding是随意整数，是否会对位置信息有影响，感觉可能会削弱位置信息。
- embedding叠加是常用做法，比如把句子所有单词叠加，貌似不太影响编码的信息
- 一般来说，embeddings也是（-1,1）之间的值

5.QANet感觉是比较好的结合了CNN和transformer的网络结构，长句子导致multi head 的矩阵运算过大的问题貌似可以通过conv1d的方法缓解

6.bert的成功基本完全是数据驱动的 
- 呃，Bert是要和TF分开讲，关于Bert请参考我这个专栏之前文章，这篇单谈TF，至于说Tf是否比RNN效果好，请参考我这篇的对比部分提到的论文实验数据

7.就实验情况而言，对于小型任务，如序列标注，效率上来说是远远比不上CNN的。直觉上，Transformer处理每一步的时候，并没有连同前面几步的结果一起处理，它只专注于当前，Transformer结构中最后用了几个卷积层，一定程度上缓解了这个问题，但效果肯定不像RNN那样具有较强的依赖。所以，保守起见，在Google更近一步的研究出来之前，还是选择用RNN或者CNN
- CNN的速度肯定没有问题，如果并行计算的话Transformer速度也不是大问题，小任务无所谓，我主要说的是公司里大规模计算的场景


8.最近的Universal Transformer 似乎解决了我说的那个问题，收敛速度已经大大加快了。我自己测试了一下，至少在序列标注任务上，BiLSTM和Universal Transformer 以基本相同的参数量同时训练了一晚上， 收敛速度几乎一样快，当然这可能和参数的选择有关，不过Universal Transformer中ACT算法的加入，应该一定程度上缓解了多步之间信息关联不强的问题


9.从直觉上self-attention在线性不重要的场景下真的是完爆了bi-RNN，比如说代码，一个方法块内的数据流控制流并不是线性的。
