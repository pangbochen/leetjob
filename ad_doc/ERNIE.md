
关于百度ERNIE及将知识图谱引入Bert

https://zhuanlan.zhihu.com/p/59503959

作者对点的arguement

# brief introduction
从目前报道的内容看，好像百度的ERNIE主要工作是：

1.预训练阶段仍旧采取 字输入，但是Mask对象是单词，如果是单纯的对单词进行Mask，我觉得这改进还好，不过我猜ERNIE很可能还专门挑出一定比例的实体词进行了连续Mask，实体词Mask我觉得是很有意义的，为啥这么感觉等会说。

2.采取了很多知识类的中文语料进行预训练，这个也挺好。



但是我觉得不应该在第一阶段预训练阶段来对"知识图谱"进行编码，第一阶段预训练阶段应该做什么，感觉GPT-2已经说明白了，就是增大数据规模，增加数据多样性，增加数据质量。量大，质好，花样多。

**大数据规模，增加数据多样性，增加数据质量**


# 推进的思考

甚至我觉得应该把Bert改成三阶段的，
- 第一个阶段LM语言模型，追求量大，质好，花样多；
- 第二个阶段，专门对各种知识图谱进行有监督的学习；
- 第三个阶段才是原来的finetuning阶段。如果能够把大量知识编码进去，我相信对于很多下游任务应该会有促进作用。

结果是词输入的效果是不如字输入的效果的

使用词输入，相对字输入，我觉得有几个缺点：一个是对分词工具有依赖，尤其是NER、新词等OOV问题，会比较影响模型效果。第二个是在预测的时候，如果是基于字的则预测结果标签集合较小，而如果是基于词的，明显标签空间会大很多，这很可能也会有劣势。

折中的方式，采用单词的方式进行折中但是

Mask采取单词的方式，我觉得算是一种折中方案，包括N-gram，也算折中方案，能比较好的平衡两者，其实是挺好的。

# 分词是否是重要的呢？
我觉得对于很多任务来说，分词是不必要的，随着Transformer的能力越来越强大，绝大多数任务应该以字作为输入，而连续的几个字是否应该是个单词，理论上应该让Transformer当做内部特征去学习，所以感觉中文分词是不必要存在的。

